{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3de5308",
   "metadata": {},
   "source": [
    "## Parts 1–4 — Batch processing for up to 27 subjects\n",
    "\n",
    "These cells reproduce Parts 1–4 of the main notebook but loop over up to 27 subject `.mat` files found in the `s2` directory. They use the exact same functions and variable names from your notebook, so you can run the notebook cells in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98477692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Imports and setup (same as your notebook)\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.ndimage import convolve1d\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Directory with subject .mat files and limit to 27 subjects\n",
    "subject_folder_name = 's2'\n",
    "subject_files = sorted([f for f in os.listdir(subject_folder_name) if f.lower().endswith('.mat')])\n",
    "subject_files = subject_files[:27]  # process up to 27 subjects\n",
    "\n",
    "print(f'Found {len(subject_files)} .mat files (using up to 27):')\n",
    "for i, f in enumerate(subject_files, start=1):\n",
    "    print(i, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3845fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 (continued): Visualize & preprocess each subject (same code as notebook)\n",
    "mov_mean_length = 25\n",
    "mov_mean_weights = np.ones(mov_mean_length) / mov_mean_length\n",
    "\n",
    "# We'll store per-subject preprocessed containers in a dict\n",
    "subjects = {}\n",
    "\n",
    "for fname in subject_files:\n",
    "    path = os.path.join(subject_folder_name, fname)\n",
    "    data = loadmat(path)\n",
    "    # Print dataset variables (skip __ fields)\n",
    "    print('\\nSubject file:', fname)\n",
    "    print('Dataset variables:')\n",
    "    for key in data.keys():\n",
    "        if not key.startswith('__'):\n",
    "            print(' ', key)\n",
    "\n",
    "    # variables named as in your notebook\n",
    "    emg = data['emg']                 # shape: (T, n_channels)\n",
    "    stimulus = data['restimulus']     # corrected labels\n",
    "    repetition = data['rerepetition'] # corrected repetition indices\n",
    "\n",
    "    print(f\"EMG data dimension : {emg.shape}\")\n",
    "    print(f\"EMG data type : {type(emg)}\")\n",
    "\n",
    "    # initialize windows & envelopes for this subject (use same method as notebook)\n",
    "    n_stimuli = len(np.unique(stimulus)) - 1\n",
    "    n_repetitions = len(np.unique(repetition)) - 1\n",
    "    emg_windows = [[None for repetition_idx in range(n_repetitions)] for stimuli_idx in range(n_stimuli)]\n",
    "    emg_envelopes = [[None for repetition_idx in range(n_repetitions)] for stimuli_idx in range(n_stimuli)]\n",
    "\n",
    "    for stimuli_idx in range(n_stimuli):\n",
    "        for repetition_idx in range(n_repetitions):\n",
    "            idx = np.logical_and(stimulus == stimuli_idx + 1, repetition == repetition_idx + 1).flatten()\n",
    "            emg_windows[stimuli_idx][repetition_idx] = emg[idx, :]\n",
    "            emg_envelopes[stimuli_idx][repetition_idx] = convolve1d(emg_windows[stimuli_idx][repetition_idx], mov_mean_weights, axis=0)\n",
    "\n",
    "    # store\n",
    "    subjects[fname] = {\n",
    "        'emg': emg,\n",
    "        'stimulus': stimulus,\n",
    "        'repetition': repetition,\n",
    "        'emg_windows': emg_windows,\n",
    "        'emg_envelopes': emg_envelopes,\n",
    "        'n_stimuli': n_stimuli,\n",
    "        'n_repetitions': n_repetitions,\n",
    "    }\n",
    "\n",
    "    # optional: plot first channel for the first subject only\n",
    "    if len(subjects) == 1:\n",
    "        plt.close('all')\n",
    "        fig, ax = plt.subplots(figsize=(8,2))\n",
    "        EMG_channel = 5 if emg.shape[1] > 5 else 0\n",
    "        ax.plot(emg[:, EMG_channel])\n",
    "        ax.set_title(f\"EMG signal channel {EMG_channel} ({fname})\")\n",
    "        ax.set_xlabel('Data points')\n",
    "        ax.set_ylabel('Amplitude')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6acdbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: build_dataset_from_ninapro function (exactly as in notebook)\n",
    "def build_dataset_from_ninapro(emg, stimulus, repetition, features=None):\n",
    "    # Calculate the number of unique stimuli and repetitions, subtracting 1 to exclude the resting condition\n",
    "    n_stimuli = np.unique(stimulus).size - 1\n",
    "    n_repetitions = np.unique(repetition).size - 1\n",
    "    # Total number of samples is the product of stimuli and repetitions\n",
    "    n_samples = n_stimuli * n_repetitions\n",
    "    \n",
    "    # Number of channels in the EMG data\n",
    "    n_channels = emg.shape[1]\n",
    "    # Calculate the total number of features by summing the number of channels for each feature\n",
    "    n_features = sum(n_channels for feature in features)\n",
    "    \n",
    "    # Initialize the dataset and labels arrays with zeros\n",
    "    dataset = np.zeros((n_samples, n_features))\n",
    "    labels = np.zeros(n_samples)\n",
    "    current_sample_index = 0\n",
    "    \n",
    "    # Loop over each stimulus and repetition to extract features\n",
    "    for i in range(n_stimuli):\n",
    "        for j in range(n_repetitions):\n",
    "            # Assign the label for the current sample\n",
    "            labels[current_sample_index] = i + 1\n",
    "            # Calculate the current sample index based on stimulus and repetition\n",
    "            current_sample_index = i * n_repetitions + j\n",
    "            current_feature_index = 0\n",
    "            # Select the time steps corresponding to the current stimulus and repetition\n",
    "            selected_tsteps = np.logical_and(stimulus == i + 1, repetition == j + 1).squeeze()\n",
    "            \n",
    "            # Loop over each feature function provided\n",
    "            for feature in features:\n",
    "                # Determine the indices in the dataset where the current feature will be stored\n",
    "                selected_features = np.arange(current_feature_index, current_feature_index + n_channels)\n",
    "                # Apply the feature function to the selected EMG data and store the result\n",
    "                dataset[current_sample_index, selected_features] = feature(emg[selected_tsteps, :])\n",
    "                # Update the feature index for the next feature\n",
    "                current_feature_index += n_channels\n",
    "\n",
    "            # Move to the next sample\n",
    "            current_sample_index += 1\n",
    "            \n",
    "    # Return the constructed dataset and corresponding labels\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9332d",
   "metadata": {},
   "source": [
    "## Part 3: Feature extraction & visualization (for each subject)\n",
    "\n",
    "We define the same features as in the notebook (MAV, STD, ...). Default features are `[mav, std]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b74abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features (same lambdas as notebook)\n",
    "mav = lambda x: np.mean(np.abs(x), axis=0)\n",
    "std = lambda x: np.std(x, axis=0)\n",
    "maxav = lambda x: np.max(np.abs(x), axis=0)\n",
    "rms = lambda x: np.sqrt(np.mean(x**2, axis=0))\n",
    "wl = lambda x: np.sum(np.abs(np.diff(x, axis=0)), axis=0)\n",
    "ssc = lambda x: np.sum((np.diff(x, axis=0)[:-1, :] * np.diff(x, axis=0)[1:, :]) < 0, axis=0)\n",
    "\n",
    "# Choose features (default to the two used in the notebook)\n",
    "features = [mav, std]\n",
    "\n",
    "# Build dataset and labels per subject using the same function\n",
    "subject_datasets = {}\n",
    "for fname, sub in subjects.items():\n",
    "    print('Building dataset for', fname)\n",
    "    dataset, labels = build_dataset_from_ninapro(\n",
    "        emg=sub['emg'],\n",
    "        stimulus=sub['stimulus'],\n",
    "        repetition=sub['repetition'],\n",
    "        features=features\n",
    "    )\n",
    "    print(f\"  dataset shape: {dataset.shape}, labels shape: {labels.shape}\")\n",
    "    subject_datasets[fname] = {'dataset': dataset, 'labels': labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2be0311",
   "metadata": {},
   "source": [
    "## Part 4: Gradient boosting classification (per subject)\n",
    "\n",
    "We run the same train/val/test split, scaling, baseline GradientBoostingClassifier fit, and evaluation (`accuracy`, `macro-F1`) that your notebook uses. Results are collected into a pandas DataFrame for the processed subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac289d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "\n",
    "for fname, data_pair in subject_datasets.items():\n",
    "    X = data_pair['dataset']\n",
    "    y = data_pair['labels']\n",
    "    print('\\nTraining subject:', fname)\n",
    "    # train/val/test split as in notebook\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.4, stratify=y, random_state=0\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=0\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_z = scaler.fit_transform(X_train)\n",
    "    X_val_z = scaler.transform(X_val)\n",
    "    X_test_z = scaler.transform(X_test)\n",
    "\n",
    "    gb = GradientBoostingClassifier(random_state=0)\n",
    "    gb.fit(X_train_z, y_train)\n",
    "\n",
    "    y_val_pred = gb.predict(X_val_z)\n",
    "    print('  Baseline val accuracy:', accuracy_score(y_val, y_val_pred))\n",
    "    print('  Baseline val macro-F1:', f1_score(y_val, y_val_pred, average='macro'))\n",
    "\n",
    "    y_test_pred = gb.predict(X_test_z)\n",
    "    baseline_acc = accuracy_score(y_test, y_test_pred)\n",
    "    baseline_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "    print('  Test accuracy:', baseline_acc)\n",
    "    print('  Test macro-F1:', baseline_f1)\n",
    "\n",
    "    # Collect results\n",
    "    results.append({\n",
    "        'subject_file': fname,\n",
    "        'baseline_acc': float(baseline_acc),\n",
    "        'baseline_f1': float(baseline_f1),\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('subject_file').reset_index(drop=True)\n",
    "print('\\nSummary for all processed subjects:')\n",
    "print(results_df)\n",
    "results_df.to_csv('results_27_subjects_part1to4.csv', index=False)\n",
    "print('\\nSaved results to results_27_subjects_part1to4.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
